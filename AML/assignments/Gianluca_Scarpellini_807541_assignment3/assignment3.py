# -*- coding: utf-8 -*-
"""assignment3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mH17rVHz5CjLWGuUOlcBvoapSX8lSl-_
"""

import numpy as np
import keras
import tensorflow as tf
from keras import models
from keras import layers
from keras.datasets import mnist
from keras import backend as K
import random

(x_train, y_train), (x_test, y_test) = mnist.load_data()
img_rows, img_cols = 28, 28

if K.image_data_format() == 'channels_first':
    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)
    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)
    input_shape = (1, img_rows, img_cols)
else:
    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)
    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)
    input_shape = (img_rows, img_cols, 1)

indexes = np.arange(x_train.shape[0])
random.seed(42)
random.shuffle(indexes)

train_val_split = int(0.8 * x_train.shape[0])
x_val = x_train[indexes[train_val_split:], :]
y_val = y_train[indexes[train_val_split:]]
x_train = x_train[indexes[:train_val_split], :]
y_train = y_train[indexes[:train_val_split]]

x_train = x_train.astype('float32')
x_val = x_val.astype('float32')
x_test = x_test.astype('float32')

x_train = x_train / 255.
x_val = x_val / 255.
x_test = x_test / 255.

# Commented out IPython magic to ensure Python compatibility.
from collections import Counter
from matplotlib import pyplot as plt
# %matplotlib inline

print('train size:', len(x_train))
print('test size:', len(x_train))
data_distribution = sorted(Counter(y_train).items())
print('labels distribution:', data_distribution)
x = np.arange(len(data_distribution))
plt.bar(x, height=[val for _, val in data_distribution])
plt.xticks(x, ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9'])
plt.show()

"""# Normalization"""

mean = np.mean(x_train)
std = np.std(x_train)
x_train = (x_train - mean) / std
x_val = (x_val - mean) / std
x_test = (x_test- mean) / std

"""# Networks"""

model = models.Sequential()
model.add(layers.Conv2D(filters=4, kernel_size=3, strides=2, input_shape=input_shape))
model.add(layers.Activation(activation='relu'))
# model.add(layers.MaxPooling2D(pool_size=(2, 2)))
# model.add(layers.BatchNormalization())


model.add(layers.Conv2D(filters=8, kernel_size=3, strides=2))
model.add(layers.Activation(activation='relu'))
# model.add(layers.MaxPooling2D(pool_size=(2, 2)))
# model.add(layers.BatchNormalization())


model.add(layers.Conv2D(filters=16, kernel_size=3, strides=2))
model.add(layers.Activation(activation='relu'))
model.add(layers.MaxPooling2D(pool_size=(2, 2)))
# model.add(layers.BatchNormalization())


model.add(layers.Flatten())
# model.add(layers.Dropout(0.15))
#model.add(layers.Dense(32, activation='relu'))
# model.add(layers.Dense(16, activation='relu'))
model.add(layers.Dense(10, activation='softmax'))
model.compile(loss=keras.losses.sparse_categorical_crossentropy, optimizer=keras.optimizers.Adam(), metrics=['accuracy'])
model.summary()

from keras.callbacks import EarlyStopping, ReduceLROnPlateau

monitor = 'loss'
early_stopping = EarlyStopping(monitor=monitor, patience=10, min_delta=0.001, restore_best_weights=True)
lr_scheduler = ReduceLROnPlateau(monitor=monitor, factor=0.1, patience=10, verbose=1)
callbacks = [early_stopping]

history = model.fit(
  x_train, 
  y_train,
  validation_data=(x_val, y_val),
  batch_size=256,
  epochs=200,
  verbose=1,
  callbacks=callbacks
)

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
# %matplotlib inline

x_plot = list(range(1, len(history.history['val_acc']) + 1))

def plot_history(history):
    plt.figure()
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.plot(x_plot, history.history['loss'])
    plt.plot(x_plot, history.history['val_loss'])
    plt.legend(['Training', 'Validation'])

    plt.figure()
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.plot(x_plot, history.history['acc'])
    plt.plot(x_plot, history.history['val_acc'])
    plt.legend(['Training', 'Validation'], loc='lower right')
    plt.show()

plot_history(history)

model.evaluate(x_test, y_test)

from sklearn.metrics import classification_report

# labels_val_from_categorical = np.argmax(labels_val, axis=1)
predictions = np.argmax(model.predict(x_test), axis=1)
print(classification_report(y_test, predictions))

